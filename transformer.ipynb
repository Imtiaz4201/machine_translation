{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb12d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required package for tokenization\n",
    "!pip install spacy\n",
    "!pip install bnlp-toolkit\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import bnlp\n",
    "from bnlp import BasicTokenizer\n",
    "import unicodedata\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b6861",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fd8a3",
   "metadata": {},
   "source": [
    "OPUS-100 (bn–en) is a large bilingual translation dataset taken from the OPUS multilingual corpus. It contains parallel sentence pairs in Bengali (bn) and English (en), where each example includes the same meaning expressed in both languages. This dataset is designed for machine translation and multilingual natural language processing tasks. When loaded with `load_dataset(\"opus100\", \"bn-en\")`, it provides training, validation, and test splits that can be used to build, tune, and evaluate translation models. The bn–en subset includes roughly one million sentence pairs collected from a wide range of open sources such as websites, subtitles, books, and public documents. This diversity helps models learn natural language patterns from real-world text, making OPUS-100 a valuable resource for Bengali–English translation research and applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"opus100\", \"bn-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "def extract_data(data, sample=None):\n",
    "    if sample is not None:\n",
    "        data = data.select(range(min(sample, len(data))))\n",
    "    eng = [d[\"translation\"][\"en\"] for d in data]\n",
    "    ban = [d[\"translation\"][\"bn\"] for d in data]\n",
    "    return ban, eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331404f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use smaller dataset for faster training\n",
    "sample_size = 50000\n",
    "train_ban, train_eng = extract_data(dataset[\"train\"], sample_size)\n",
    "val_ban, val_eng = extract_data(dataset[\"validation\"], sample_size // 10)\n",
    "test_ban, test_eng = extract_data(dataset[\"test\"], sample_size // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68631faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train: {len(train_ban)} samples\")\n",
    "print(f\"Validation: {len(val_ban)} samples\")\n",
    "print(f\"Test: {len(test_ban)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf8462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing classes\n",
    "class ProcessBengaliCorpus:\n",
    "    def __init__(self) -> None:\n",
    "        self.data = None\n",
    "        self.tokenizer = BasicTokenizer()\n",
    "\n",
    "    def clean_data(self, data):\n",
    "        # Remove English characters and numbers for pure Bengali\n",
    "        self.data = list(map(lambda x: re.sub(r\"[a-zA-Z0-9\\()\\_\\-]\", \"\", x), data))\n",
    "\n",
    "        # Remove spaces around hasanta (্) (U+09CD)\n",
    "        self.data = list(map(lambda x: re.sub(r\"\\s*\\u09cd\\s*\", \"\\u09cd\", x), self.data))\n",
    "\n",
    "        # Remove standalone hasanta (্)\n",
    "        self.data = list(map(lambda x: re.sub(r\"\\s+\\u09cd\", \"\", x), self.data))\n",
    "        self.data = list(map(lambda x: re.sub(r\"\\u09cd\\s+\", \"\", x), self.data))\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        self.data = list(map(lambda x: re.sub(r\"\\s+\", \" \", x), self.data))\n",
    "\n",
    "        # Standardize punctuation\n",
    "        self.data = [i.replace(\"ঃ\", \":\") for i in self.data]\n",
    "        self.data = [i.replace(\"।\", \".\") for i in self.data]\n",
    "\n",
    "        self.data = [i.strip() for i in self.data]\n",
    "\n",
    "        # Normalize Unicode\n",
    "        self.data = [unicodedata.normalize(\"NFC\", i) for i in self.data]\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def tokenize_bengla(self, sen):\n",
    "        # Tokenize Bengali text\n",
    "        tokenized = self.tokenizer.tokenize(sen.strip())\n",
    "        token = [str(t).strip() for t in tokenized if str(t).strip()]\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b9386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessEnglishCorpus:\n",
    "    def __init__(self) -> None:\n",
    "        self.data = None\n",
    "        self.eng_tokenizer = spacy.load(\n",
    "            \"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"]\n",
    "        )\n",
    "\n",
    "    def clean_data(self, data):\n",
    "        # Lowercase each word\n",
    "        self.data = [i.lower() for i in data]\n",
    "\n",
    "        # Keep basic punctuation\n",
    "        self.data = list(map(lambda x: re.sub(r\"[^\\w\\s\\.\\?\\!,']\", \"\", x), self.data))\n",
    "\n",
    "        # Remove extra spaces\n",
    "        self.data = list(map(lambda x: re.sub(r\"\\s+\", \" \", x), self.data))\n",
    "\n",
    "        self.data = [i.strip() for i in self.data]\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def tokenize_english(self, text):\n",
    "        # Tokenize English text\n",
    "        tokenized = self.eng_tokenizer(text.strip())\n",
    "        token = [t.text.lower() for t in tokenized]\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12760a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate preprocessors\n",
    "bn_proc = ProcessBengaliCorpus()\n",
    "en_proc = ProcessEnglishCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed4a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all datasets\n",
    "train_bn_clean = bn_proc.clean_data(train_ban)\n",
    "train_en_clean = en_proc.clean_data(train_eng)\n",
    "val_bn_clean = bn_proc.clean_data(val_ban)\n",
    "val_en_clean = en_proc.clean_data(val_eng)\n",
    "test_bn_clean = bn_proc.clean_data(test_ban)\n",
    "test_en_clean = en_proc.clean_data(test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lists(str_list, lang=\"bn\"):\n",
    "    \"\"\"Tokenize source and target\"\"\"\n",
    "    if lang == \"bn\":\n",
    "        return [bn_proc.tokenize_bengla(i) for i in str_list]\n",
    "    else:\n",
    "        return [en_proc.tokenize_english(i) for i in str_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61218d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "train_bn_toks = tokenize_lists(train_bn_clean, lang=\"bn\")\n",
    "train_en_toks = tokenize_lists(train_en_clean, lang=\"en\")\n",
    "val_bn_toks = tokenize_lists(val_bn_clean, lang=\"bn\")\n",
    "val_en_toks = tokenize_lists(val_en_clean, lang=\"en\")\n",
    "test_bn_toks = tokenize_lists(test_bn_clean, lang=\"bn\")\n",
    "test_en_toks = tokenize_lists(test_en_clean, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary class\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.specials = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "\n",
    "    def vocab_builder(self, data, max_size=30000, min_freq=2):\n",
    "        counter = Counter()\n",
    "        for sent in data:\n",
    "            counter.update(sent)\n",
    "\n",
    "        # Filter tokens by min_freq and exclude any special tokens that may appear in corpus\n",
    "        words = [\n",
    "            w for w, f in counter.items() if f > min_freq and w not in self.specials\n",
    "        ]\n",
    "        # Sort max to min\n",
    "        words = sorted(words, key=lambda w: counter[w], reverse=True)\n",
    "        \n",
    "        # Reserve space for specials\n",
    "        if max_size:\n",
    "            words = words[: max_size - len(self.specials)]\n",
    "\n",
    "        # mappings\n",
    "        self.itos = list(self.specials) + words # main vocabulary \n",
    "        self.stoi = {w: i for i, w in enumerate(self.itos)} # word2key\n",
    "\n",
    "        self.pad_idx = self.stoi[\"<pad>\"]\n",
    "        self.sos_idx = self.stoi[\"<sos>\"]\n",
    "        self.eos_idx = self.stoi[\"<eos>\"]\n",
    "        self.unk_idx = self.stoi[\"<unk>\"]\n",
    "\n",
    "    def get_itos(self):\n",
    "        return self.itos\n",
    "\n",
    "    def get_stoi(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def encode(self, tokens, add_eos=True):\n",
    "        #return indx of each token from main vocabulary\n",
    "        ids = [self.stoi.get(t, self.unk_idx) for t in tokens]\n",
    "        if add_eos:\n",
    "            # add <eos> only for target\n",
    "            ids.append(self.eos_idx)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # get words based on ids\n",
    "        out = []\n",
    "        for i in ids:\n",
    "            tok = self.itos[i] if 0 <= i < len(self.itos) else \"<unk>\"\n",
    "            if tok in (\"<eos>\", \"<pad>\"):\n",
    "                break\n",
    "            if tok == \"<sos>\":\n",
    "                continue\n",
    "            out.append(tok)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac870927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies from training data\n",
    "src_vocab = Vocab()\n",
    "src_vocab.vocab_builder(train_en_toks, max_size=50000, min_freq=2)\n",
    "tgt_vocab = Vocab()\n",
    "tgt_vocab.vocab_builder(train_bn_toks, max_size=50000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_en_toks[0]\n",
    "en = src_vocab.encode(t)\n",
    "de = src_vocab.decode(en)\n",
    "print(f\"text= {t}\")\n",
    "print(f\"encoded= {en}\")\n",
    "print(f\"decoded= {de}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b14dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = train_bn_toks[0]\n",
    "en = tgt_vocab.encode(t)\n",
    "de = tgt_vocab.decode(en)\n",
    "print(f\"text= {t}\")\n",
    "print(f\"encoded= {en}\")\n",
    "print(f\"decoded= {de}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51126f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89849d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class BengaliEnglishDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, src_token_lists, tgt_token_lists, src_vocab, tgt_vocab, max_len=50\n",
    "    ):\n",
    "        self.src_token_lists = src_token_lists\n",
    "        self.tgt_token_lists = tgt_token_lists\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_token_lists)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tokens = self.src_token_lists[idx]\n",
    "        tgt_tokens = self.tgt_token_lists[idx]\n",
    "\n",
    "        # Encode with special tokens\n",
    "        src_ids = [self.src_vocab.sos_idx] + self.src_vocab.encode(\n",
    "            src_tokens, add_eos=False\n",
    "        )\n",
    "        tgt_ids = [self.tgt_vocab.sos_idx] + self.tgt_vocab.encode(\n",
    "            tgt_tokens, add_eos=False\n",
    "        )\n",
    "\n",
    "        # Truncate if too long\n",
    "        if len(src_ids) > self.max_len:\n",
    "            src_ids = src_ids[: self.max_len - 1] + [self.src_vocab.eos_idx]\n",
    "        else:\n",
    "            src_ids = src_ids + [self.src_vocab.eos_idx]\n",
    "\n",
    "        if len(tgt_ids) > self.max_len:\n",
    "            tgt_ids = tgt_ids[: self.max_len - 1] + [self.tgt_vocab.eos_idx]\n",
    "        else:\n",
    "            tgt_ids = tgt_ids + [self.tgt_vocab.eos_idx]\n",
    "\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(\n",
    "            tgt_ids, dtype=torch.long\n",
    "        )\n",
    "\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch):\n",
    "    src_list, tgt_list = [], []\n",
    "    for src, tgt in batch:\n",
    "        src_list.append(src)\n",
    "        tgt_list.append(tgt)\n",
    "\n",
    "    src_padded = pad_sequence(\n",
    "        src_list, batch_first=True, padding_value=src_vocab.pad_idx\n",
    "    )\n",
    "    tgt_padded = pad_sequence(\n",
    "        tgt_list, batch_first=True, padding_value=tgt_vocab.pad_idx\n",
    "    )\n",
    "\n",
    "    return src_padded, tgt_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d2261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "max_len = 50\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = BengaliEnglishDataset(\n",
    "    train_en_toks, train_bn_toks, src_vocab, tgt_vocab, max_len=max_len\n",
    ")\n",
    "val_dataset = BengaliEnglishDataset(\n",
    "    val_en_toks, val_bn_toks, src_vocab, tgt_vocab, max_len=max_len\n",
    ")\n",
    "test_dataset = BengaliEnglishDataset(\n",
    "    test_en_toks, test_bn_toks, src_vocab, tgt_vocab, max_len=max_len\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Source vocabulary size: {len(src_vocab.itos)}\")\n",
    "print(f\"Target vocabulary size: {len(tgt_vocab.itos)}\")\n",
    "print(\n",
    "    f\"Special tokens - PAD: {src_vocab.pad_idx}, SOS: {src_vocab.sos_idx}, EOS: {src_vocab.eos_idx}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88da6e00",
   "metadata": {},
   "source": [
    "# TRANSFORMER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccea403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention Layer\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads # Size per head\n",
    "\n",
    "        # Linear layers to create Query, Key, Value\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        # Final output projection\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "       \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "       \n",
    "        # Scaling factor to stabilize gradients\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Project inputs into Q, K, V\n",
    "        Q = self.fc_q(query) # (batch, query_len, hid_dim)\n",
    "        K = self.fc_k(key) # (batch, key_len,   hid_dim)\n",
    "        V = self.fc_v(value) # (batch, value_len, hid_dim)\n",
    "\n",
    "        # Split into heads and rearrange:\n",
    "        # (batch, seq_len, hid_dim) → (batch, heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # energy shape: (batch, heads, query_len, key_len)\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V) # (batch, heads, query_len, head_dim)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        return x, attention\n",
    "\n",
    "\n",
    "# Position-wise Feedforward Layer\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim) # First linear layer expands the hidden dimension\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim) # Second linear layer projects back to hidden size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc1(x))) # (batch, seq_len, pf_dim)\n",
    "        x = self.fc2(x) # (batch, seq_len, hid_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device) # Multi-head self-attention\n",
    "        self.feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout) # Multi-head self-attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # Self-Attention Block\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src)) # Residual connection + layer normalization\n",
    "        # Feedforward Block\n",
    "        _src = self.feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src)) # Residual connection + layer normalization\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hid_dim,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        pf_dim,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length=100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Token embedding: converts word index → vector\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        # Positional embedding: adds position information\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        # Stack of encoder layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = (\n",
    "            torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        ) # (batch, seq_len)\n",
    "        src = self.dropout(\n",
    "            (self.tok_embedding(src) * self.scale) + self.pos_embedding(pos)\n",
    "        )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        # Layer norms for each sub-layer\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        # Self-attention\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        # Encoder-decoder attention\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(\n",
    "            hid_dim, n_heads, dropout, device\n",
    "        )\n",
    "\n",
    "        self.feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        _trg = self.feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        return trg, attention\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        hid_dim,\n",
    "        n_layers,\n",
    "        n_heads,\n",
    "        pf_dim,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length=100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = (\n",
    "            torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        )\n",
    "        trg = self.dropout(\n",
    "            (self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos)\n",
    "        )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "        return output, attention\n",
    "\n",
    "\n",
    "# Seq2Seq Transformer\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(\n",
    "            torch.ones((trg_len, trg_len), device=self.device)\n",
    "        ).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "\n",
    "# Initialize weights\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, \"weight\") and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b29ab2",
   "metadata": {},
   "source": [
    "## TRAINING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0 # To track total loss for this epoch\n",
    "\n",
    "    for src, trg in loader:\n",
    "        # Loop over each batch from the DataLoader\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output, _ = model(src, trg[:, :-1])\n",
    "        output_dim = output.shape[-1]\n",
    "        # Flatten predictions: (batch, seq_len, vocab) → (batch*seq_len, vocab)\n",
    "        output = output.reshape(-1, output_dim)\n",
    "\n",
    "        # Shift target: remove first token (usually <sos>)\n",
    "        # and flatten to match output shape\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        # Compute loss between predicted tokens and real tokens\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Disable gradient calculation\n",
    "        for src, trg in loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "            # Forward pass (no teacher forcing)\n",
    "            output, _ = model(src, trg[:, :-1])\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            # Compute validation loss\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d18d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src_vocab.itos)\n",
    "OUTPUT_DIM = len(tgt_vocab.itos)\n",
    "HID_DIM = 128\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "SRC_PAD_IDX = src_vocab.pad_idx\n",
    "TRG_PAD_IDX = tgt_vocab.pad_idx\n",
    "\n",
    "# Create encoder and decoder\n",
    "enc = Encoder(\n",
    "    INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device\n",
    ")\n",
    "dec = Decoder(\n",
    "    OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37e8722",
   "metadata": {},
   "source": [
    "## TRAINING TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_ppl = []\n",
    "val_ppl = []\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_ppl.append(math.exp(train_loss))\n",
    "    val_ppl.append(math.exp(valid_loss))\n",
    "\n",
    "    # save model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"best-transformer-model.pt\")\n",
    "\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "        print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\")\n",
    "\n",
    "end_time =  time.perf_counter() - start_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg. training loss : {sum(train_losses)/N_EPOCHS}\")\n",
    "print(f\"Avg. training ppl loss : {sum(train_ppl)/N_EPOCHS}\")\n",
    "print(f\"Avg. validation loss : {sum(valid_losses)/N_EPOCHS}\")\n",
    "print(f\"Avg. validation ppl loss : {sum([val_ppl])/N_EPOCHS}\")\n",
    "print(f\"Training time(ms) : {end_time * 1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bb8da",
   "metadata": {},
   "source": [
    "<h4>Tranining results</h4>\n",
    "<img src=\"trans.png\" alt=\"Tranining results\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9163d",
   "metadata": {},
   "source": [
    "<h4>Training and Validation Loss results</h4>\n",
    "<img src=\"transTVLOSS.png\" alt=\"Training and Validation Loss results\" width=500 height=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69afee03",
   "metadata": {},
   "source": [
    "<h4>Test results</h4>\n",
    "<img src=\"transTest.png\" alt=\"Test results\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a1ce9",
   "metadata": {},
   "source": [
    "<h4>Attention heatmap (Head 0)</h4>\n",
    "<img src=\"transATVH.png\" alt=\"Test results\" width=500 height=300>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a0709",
   "metadata": {},
   "source": [
    "# SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501020cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"hparams\": {\n",
    "        \"input_dim\": INPUT_DIM,\n",
    "        \"output_dim\": OUTPUT_DIM,\n",
    "        \"hid_dim\": HID_DIM,\n",
    "        \"enc_layers\": ENC_LAYERS,\n",
    "        \"dec_layers\": DEC_LAYERS,\n",
    "        \"enc_heads\": ENC_HEADS,\n",
    "        \"dec_heads\": DEC_HEADS,\n",
    "        \"enc_pf_dim\": ENC_PF_DIM,\n",
    "        \"dec_pf_dim\": DEC_PF_DIM,\n",
    "        \"enc_dropout\": ENC_DROPOUT,\n",
    "        \"dec_dropout\": DEC_DROPOUT,\n",
    "        \"src_pad_idx\": SRC_PAD_IDX,\n",
    "        \"trg_pad_idx\": TRG_PAD_IDX,\n",
    "        \"max_length\": max_len        # important: sequence length used for padding / pos enc\n",
    "\n",
    "    },\n",
    "    \"src_itos\": src_vocab.get_itos(),\n",
    "    \"tgt_itos\": tgt_vocab.get_itos(),\n",
    "    \"src_stoi\": src_vocab.get_stoi(),   # optional (you can rebuild from itos)\n",
    "    \"tgt_stoi\": tgt_vocab.get_stoi(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"transformer_checkpoint.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af6077",
   "metadata": {},
   "source": [
    "## TESTING TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best-transformer-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e658180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nTest Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22109bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "path = os.path.join(os.getcwd(), \"app\", \"saved_models\")\n",
    "\n",
    "CKPT_PATH = os.path.join(path, \"transformer_checkpoint.pt\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) load checkpoint\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "h = ckpt[\"hparams\"]\n",
    "\n",
    "# 2) rebuild encoder/decoder exactly as at training\n",
    "enc = Encoder(\n",
    "    h[\"input_dim\"],\n",
    "    h[\"hid_dim\"],\n",
    "    h[\"enc_layers\"],\n",
    "    h[\"enc_heads\"],\n",
    "    h[\"enc_pf_dim\"],\n",
    "    h[\"enc_dropout\"],\n",
    "    device,\n",
    ")\n",
    "\n",
    "dec = Decoder(\n",
    "    h[\"output_dim\"],\n",
    "    h[\"hid_dim\"],\n",
    "    h[\"dec_layers\"],\n",
    "    h[\"dec_heads\"],\n",
    "    h[\"dec_pf_dim\"],\n",
    "    h[\"dec_dropout\"],\n",
    "    device,\n",
    ")\n",
    "\n",
    "# 3) seq2seq wrapper\n",
    "model = Seq2SeqTransformer(enc, dec, h[\"src_pad_idx\"], h[\"trg_pad_idx\"], device)\n",
    "\n",
    "# 4) load state_dict and move to device\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()   # ensure eval mode\n",
    "\n",
    "# 5) rebuild vocabs\n",
    "src_vocab = Vocab()\n",
    "tgt_vocab = Vocab()\n",
    "\n",
    "src_vocab.itos = ckpt[\"src_itos\"]\n",
    "tgt_vocab.itos = ckpt[\"tgt_itos\"]\n",
    "\n",
    "src_vocab.stoi = {w: i for i, w in enumerate(src_vocab.itos)}\n",
    "tgt_vocab.stoi = {w: i for i, w in enumerate(tgt_vocab.itos)}\n",
    "\n",
    "src_vocab.pad_idx = src_vocab.stoi[\"<pad>\"]\n",
    "src_vocab.sos_idx = src_vocab.stoi[\"<sos>\"]\n",
    "src_vocab.eos_idx = src_vocab.stoi[\"<eos>\"]\n",
    "src_vocab.unk_idx = src_vocab.stoi[\"<unk>\"]\n",
    "\n",
    "tgt_vocab.pad_idx = tgt_vocab.stoi[\"<pad>\"]\n",
    "tgt_vocab.sos_idx = tgt_vocab.stoi[\"<sos>\"]\n",
    "tgt_vocab.eos_idx = tgt_vocab.stoi[\"<eos>\"]\n",
    "tgt_vocab.unk_idx = tgt_vocab.stoi[\"<unk>\"]\n",
    "\n",
    "def translate_sentence(sentence, model, src_vocab, tgt_vocab, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and encode source sentence\n",
    "    tokens = en_proc.tokenize_english(sentence)\n",
    "    tokens = (\n",
    "        [src_vocab.sos_idx]\n",
    "        + src_vocab.encode(tokens, add_eos=False)\n",
    "        + [src_vocab.eos_idx]\n",
    "    )\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    # Create source mask\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    # Initialize target with <sos> token\n",
    "    trg_indexes = [tgt_vocab.sos_idx]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == tgt_vocab.eos_idx:\n",
    "            break\n",
    "\n",
    "    # Decode target tokens\n",
    "    trg_tokens = tgt_vocab.decode(trg_indexes)\n",
    "\n",
    "    return \" \".join(trg_tokens), attention\n",
    "\n",
    "\n",
    "# 6) call your translate function (unchanged)\n",
    "english_sentence = \"the shop is closed at night\"\n",
    "translation, attention = translate_sentence(\n",
    "    english_sentence, model, src_vocab, tgt_vocab, device, max_len=h.get(\"max_length\", 50)\n",
    ")\n",
    "\n",
    "print(\"EN:\", english_sentence)\n",
    "print(\"BN:\", translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22405c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, src_vocab, tgt_vocab, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and encode source sentence\n",
    "    tokens = en_proc.tokenize_english(sentence)\n",
    "    tokens = (\n",
    "        [src_vocab.sos_idx]\n",
    "        + src_vocab.encode(tokens, add_eos=False)\n",
    "        + [src_vocab.eos_idx]\n",
    "    )\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "\n",
    "    # Create source mask\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    # Initialize target with <sos> token\n",
    "    trg_indexes = [tgt_vocab.sos_idx]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == tgt_vocab.eos_idx:\n",
    "            break\n",
    "\n",
    "    # Decode target tokens\n",
    "    trg_tokens = tgt_vocab.decode(trg_indexes)\n",
    "\n",
    "    return \" \".join(trg_tokens), attention\n",
    "\n",
    "\n",
    "example_idx = 7\n",
    "example_src = test_eng[example_idx]\n",
    "example_tgt = test_ban[example_idx]\n",
    "\n",
    "print(f\"\\nExample {example_idx + 1}:\")\n",
    "print(f\"Source (EN): {example_src}\")\n",
    "print(f\"Target (BN): {example_tgt}\")\n",
    "\n",
    "# Clean and tokenize for translation\n",
    "clean_src = en_proc.clean_data([example_src])[0]\n",
    "translation, attention = translate_sentence(\n",
    "    clean_src, model, src_vocab, tgt_vocab, device\n",
    ")\n",
    "print(f\"translation (BN): {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING LOSS\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(valid_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ATTENTION VISUALIZATION \n",
    "def display_attention(sentence, translation, attention, n_heads=8, head=0):\n",
    "    \"\"\"\n",
    "    Display attention for a specific head\n",
    "    \"\"\"\n",
    "    if attention.dim() == 4:\n",
    "        attention = attention.squeeze(0)[head]  # Take first head\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Get tokens\n",
    "    src_tokens = [\"<sos>\"] + en_proc.tokenize_english(sentence) + [\"<eos>\"]\n",
    "    trg_tokens = [\"<sos>\"] + translation.split() + [\"<eos>\"]\n",
    "\n",
    "    # Create heatmap\n",
    "    cax = ax.matshow(attention.cpu().detach().numpy(), cmap=\"viridis\")\n",
    "\n",
    "    # Set ticks\n",
    "    ax.set_xticks(range(len(src_tokens)))\n",
    "    ax.set_yticks(range(len(trg_tokens)))\n",
    "    ax.set_xticklabels(src_tokens, rotation=45)\n",
    "    ax.set_yticklabels(trg_tokens)\n",
    "\n",
    "    ax.set_xlabel(\"Source Tokens\")\n",
    "    ax.set_ylabel(\"Target Tokens\")\n",
    "    ax.set_title(f\"Attention Visualization (Head {head})\")\n",
    "\n",
    "    plt.colorbar(cax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize attention for the example\n",
    "if attention is not None:\n",
    "    display_attention(clean_src, translation, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
